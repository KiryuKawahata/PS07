---
title: "STAT/MATH 495: Problem Set 07"
author: "Kiryu Kawahata"
date: "2017-10-24"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    collapsed: false
    smooth_scroll: false
    df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, fig.width=8, fig.height=4.5, message=FALSE, warning = FALSE
  )
set.seed(76)

# Load packages
library(tidyverse)
library(broom)
library(knitr)
library(corrplot)
library(ROCR)

train <- read_csv("data/cs-training.csv") %>% 
  rename(Id = X1)
test <- read_csv("data/cs-test.csv") %>% 
  rename(Id = X1)
submission <- read_csv("data/sampleEntry.csv")
```

Information on the competition can be found [here](https://www.kaggle.com/c/GiveMeSomeCredit/data).



# Collaboration

Please indicate who you collaborated with on this assignment: 

# Exploration

```{r}


#Visual analysis

kable(cor(train[c(4, 6, 7)], train$SeriousDlqin2yrs), title="Correlation with Count")

train_VOI <- train %>%
  select(DebtRatio, age, MonthlyIncome, SeriousDlqin2yrs)

null <- lm(SeriousDlqin2yrs ~ 1, data = train_VOI)
full <- lm(SeriousDlqin2yrs ~ ., data = train_VOI)
step(null, scope = list(lower = null, upper = full), direction = "both", steps = 3)


ggplot(data = train, aes(x = age))+ geom_histogram() + theme(legend.position="none") + labs(title = "Histogram for Age")

ggplot(data = train, aes(x = MonthlyIncome))+ geom_histogram() + theme(legend.position="none") + labs(title = "Histogram of Monthly Income ($30,000/month or less)", x = "Monthly Income") + xlim(0, 30000)

ggplot(data = train, aes(x = DebtRatio))+ geom_histogram() + theme(legend.position="none") + labs(title = "Histogram of Debt Ratio", x = "Debt Ratio") + xlim(0, 30000) + ylim(0, 3500)


```

Having the greatest absolute correlation value, as well as being the most influential in stepwise regression, age appears to be the "strongest" predictor. In addition the distribution for age appears to be the most "normal" of the three. When looking at Monthly Income there seems to be a trend, however there seems to be a significantly lower representation of higher income levels in this training set. Only 792 of the sample earn over 30000 a month. I'm not confident that filtering the training set will create accurate predictions for all incomes, so I've chosen age to be the sole predictor here. 


# Build binary classifier

Build the binary classifier based on a single predictor variable: `DebtRatio`,
`age`, or `MonthlyIncome`. Justify this choice.

```{r}
PS_formula <- as.formula(SeriousDlqin2yrs ~ age)
PS_Model <- glm(PS_formula, data = train, family = "binomial")

PS_Predictions <- PS_Model %>% 
  broom::augment(newdata = test) %>% 
  as_tibble() %>% 
  mutate(P_hat = 1/(1 + exp(-.fitted)))

test_submission <- submission %>% 
  mutate(Probability = PS_Predictions$P_hat)
  
write.csv(test_submission, "submission.csv", row.names = F)
```



# ROC curve

Based on the ultimate classifier you choose, plot a corresponding ROC curve.

```{r}

PS_augmented <- PS_Model %>% 
  broom::augment() %>% 
  as_tibble() %>% 
  mutate(P_hat = 1/(1+exp(-.fitted)))

PS_pred <- prediction(predictions = PS_augmented$P_hat, labels = PS_augmented$SeriousDlqin2yrs)
PS_perf <- performance(PS_pred, "tpr","fpr")


auc <- as.numeric(performance(PS_pred,"auc")@y.values)
auc

plot(PS_perf, main=paste("Area Under the Curve =", round(auc, 3)))
abline(c(0, 1), lty=2)
```



# ROC curve for random guessing

Instead of using any predictor information as you did above, switch your
predictions to random guesses and plot the resulting ROC curve.

```{r}
PS_random <- PS_Model %>% 
  broom::augment() %>% 
  as_tibble() %>% 
  mutate(P_hat = 0.5)

PS_pred_Random <- prediction(predictions = PS_random$P_hat, labels = PS_random$SeriousDlqin2yrs)
perf_Rand <- performance(PS_pred_Random, "tpr","fpr")


auc_rand <- as.numeric(performance(PS_pred_Random,"auc")@y.values)
auc_rand

plot(perf_Rand, main=paste("Area Under the Curve =", round(auc_rand, 3)))

```